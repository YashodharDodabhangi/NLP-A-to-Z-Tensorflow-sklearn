# -*- coding: utf-8 -*-
"""Custom_call_backs_in_Tensorflow.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1vXBAF43Dq9xToAgjIUG55FmZZ92pPkNK
"""

import tensorflow as tf
import pandas as pd 
import numpy as np

from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense,Input,Activation,Flatten


from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.callbacks import LearningRateScheduler


from tensorflow.keras.callbacks import TensorBoard

dataset=tf.keras.datasets.fashion_mnist

(train_images,train_lables),(test_images,test_lables)=dataset.load_data()

train_lables.shape

train_images.shape

train_images=train_images/255
test_images=test_images/255

from tensorflow.keras.callbacks import ModelCheckpoint
from tensorflow.keras.callbacks import EarlyStopping
from tensorflow.keras.callbacks import LearningRateScheduler

from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()


from sklearn.metrics.pairwise import pairwise_distances_argmin_min
import os
from sklearn.metrics import precision_score, \
    recall_score, confusion_matrix, \
    accuracy_score, f1_score,roc_auc_score

from tensorflow.keras.callbacks import TensorBoard


class callback_class(tf.keras.callbacks.Callback):
  def __init__(self,validation_data):
      self.x_test = validation_data[0]
      self.y_test= validation_data[1]
  def on_train_begin(self, logs={}):
      pass
      
  def on_epoch_end(self, epoch, logs={}):
        #step4(printing loss and is it is not decressing then stop the model training)
        loss = logs.get('loss')
        if loss is not None:
            if np.isnan(loss) or np.isinf(loss):
                print("Invalid loss and terminated at epoch {}".format(epoch))
                self.model.stop_training = True

        model_weights = self.model.get_weights()
        if model_weights is not None:
          if np.any([np.any(np.isnan(x)) for x in model_weights]):
            print("Invalid weight and terminated at epoch {}".format(epoch))
            self.model.stop_training = True


history_own=callback_class(validation_data=[test_images,test_lables]) 


def scheduler(epoch,lr):  
  if((epoch+1)%3==0):     
    lr=0.95*lr 
    return lr


#step3
reduce_lr =tf.keras.callbacks.ReduceLROnPlateau(monitor='accuracy', factor=0.1,patience=1, min_lr=0.0005,mode='max')
earlystop = EarlyStopping(monitor='accuracy', min_delta=0.05, patience=2, verbose=1,mode='max')

####Optimizer
optimizer=tf.keras.optimizers.SGD(
    learning_rate=0.01,
    momentum=0.2,
    nesterov=False,
    name='SGD',
  
)

### Initilizations
Initilizer=tf.keras.initializers.RandomUniform(
    minval=0, maxval=1, seed=None
)

#model architecture
input_layer=Input(shape=(28,28))
layer=Flatten()(input_layer)
layer1 = Dense(50,activation='relu',kernel_initializer=Initilizer)(layer)
layer2 = Dense(50,activation='relu',kernel_initializer=Initilizer)(layer1)
layer3 = Dense(40,activation='relu',kernel_initializer=Initilizer)(layer2)
layer4 = Dense(30,activation='relu',kernel_initializer=Initilizer)(layer3)
output = Dense(10,activation='softmax',kernel_initializer=Initilizer)(layer4)

model=Model(inputs=input_layer,outputs=output)

model.compile(optimizer=optimizer,loss="sparse_categorical_crossentropy",metrics=['accuracy'])

model.fit(train_images, train_lables, epochs=10, batch_size=32,callbacks=[history_own,earlystop,reduce_lr])



