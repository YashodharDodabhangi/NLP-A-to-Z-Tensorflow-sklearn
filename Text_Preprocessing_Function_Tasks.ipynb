{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HNhOKGm1vVSf"
      },
      "outputs": [],
      "source": [
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Removing the Short forms in English\n",
        "\n",
        "\n",
        "def decontracted(phrase):\n",
        "    # specific\n",
        "    phrase = re.sub(r\"won't\", \"will not\", phrase)\n",
        "    phrase = re.sub(r\"can\\'t\", \"can not\", phrase)\n",
        "\n",
        "    # general\n",
        "    phrase = re.sub(r\"n\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'re\", \" are\", phrase)\n",
        "    phrase = re.sub(r\"\\'s\", \" is\", phrase)\n",
        "    phrase = re.sub(r\"\\'d\", \" would\", phrase)\n",
        "    phrase = re.sub(r\"\\'ll\", \" will\", phrase)\n",
        "    phrase = re.sub(r\"\\'t\", \" not\", phrase)\n",
        "    phrase = re.sub(r\"\\'ve\", \" have\", phrase)\n",
        "    phrase = re.sub(r\"\\'m\", \" am\", phrase)\n",
        "    return phrase"
      ],
      "metadata": {
        "id": "-1IH-pn3wRJL"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove words with numbers python: https://stackoverflow.com/a/18082370/4084039\n",
        "output = re.sub(\"\\S*\\d\\S*\", \"\", sent_text).strip()\n",
        "print(output)"
      ],
      "metadata": {
        "id": "LkMgd-Sn76M7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove spacial character: https://stackoverflow.com/a/5843547/4084039\n",
        "sent = re.sub('[^A-Za-z0-9]+', ' ', sent)\n",
        "print(sent)"
      ],
      "metadata": {
        "id": "csi4cHuL8Mcd"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install NLTK"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t_2yuaHsByYD",
        "outputId": "0154a8fc-006e-4143-e1b4-810b9e7b2d01"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: NLTK in /usr/local/lib/python3.8/dist-packages (3.7)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from NLTK) (4.64.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.8/dist-packages (from NLTK) (2022.6.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.8/dist-packages (from NLTK) (1.2.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.8/dist-packages (from NLTK) (7.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#remove stop words from text corpus\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "\n",
        "def remove_stopwords(text):\n",
        "  sentence=text\n",
        "  li=[]\n",
        "  for i in sentance.split(\" \"):\n",
        "    if i.lower() not in set(stopwords.words('english')):\n",
        "      li.append(i)\n",
        "  return li\n",
        "      \n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WyIhhzJT8NYW",
        "outputId": "de09b5bb-728d-400f-e75c-a24cc493e672"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text=\"This is a sample sentence,showing off the stop words filtration.\"\n",
        "op=remove_stopwords(text)\n",
        "op"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2TRNHWrS-Hsb",
        "outputId": "6d7ba634-dfc4-4d95-c9d4-465c980af4e1"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['sample', 'sentence,showing', 'stop', 'words', 'filtration.']"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#for using pysprk : https://stackoverflow.com/questions/53579444/efficient-text-preprocessing-using-pyspark-clean-tokenize-stopwords-stemming"
      ],
      "metadata": {
        "id": "V_O5t1tj-WWR"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#library that contains punctuation\n",
        "import string\n",
        "string.punctuation"
      ],
      "metadata": {
        "id": "GkCpP8ckAq3M"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#defining the function to remove punctuation\n",
        "def remove_punctuation(text):\n",
        "    punctuationfree=\"\".join([i for i in text if i not in string.punctuation])\n",
        "    return punctuationfree"
      ],
      "metadata": {
        "id": "Drg4zdtdGO5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#importing the Stemming function from nltk library\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "#defining the object for stemming\n",
        "porter_stemmer = PorterStemmer()\n",
        "#defining a function for stemming\n",
        "def stemming(text):\n",
        "stem_text = [porter_stemmer.stem(word) for word in text]\n",
        "    return stem_text"
      ],
      "metadata": {
        "id": "51auWCIhpVnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "#defining the object for Lemmatization\n",
        "wordnet_lemmatizer = WordNetLemmatizer()\n",
        "#defining the function for lemmatization\n",
        "def lemmatizer(text):\n",
        "lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n",
        "    return lemm_text"
      ],
      "metadata": {
        "id": "1HOiU4gkpsER"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#In the first row- crazy has been changed to crazi which has no meaning but for lemmatization, it remained the same i.e crazy\n",
        "\n",
        "#In the last row- goes has changed to goe while stemming but for lemmatization, it has converted into go which is meaningful."
      ],
      "metadata": {
        "id": "MUtvSjE_p8_x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}